\chapter{Analysis}
\label{chapter:analysis}

In \autoref{cha:prior-work}, we discussed how the project started as a decentralized
storage system with verification and then evolved into integrating PoR into the verification,
as soon as limitations of the decentralized verification mechanism became apparent.
In this chapter we discuss how we are solving the problem of storage attacks
and how PoR integrates into the reputation system.
It is worth noting that apart from malicious peers, it is also possible for well-behaved peers to
lose data due to hardware failures and bit rot.
We do not cover this case as a special case, because it is a light version of what malicious
peers are capable of doing.

\section{Malicious peers}

There could be many types of malicious peers in a decentralized network,
but we cover only the relevant ones for the scope of the project.
The Related Work \autoref{chapter:related-work} covers other systems and
how they handle the other types of attacks, not covered here.

There are two types of scenarios that can affect the integrity of the data in the network:
\begin{enumerate}
    \item \textbf{Data availability degradation} --- an attacker claims to store data, but does not.
    \item \textbf{Data integrity degradation} --- an attacker claims to store data, but stores different data, or
        data that has become corrupted without the peer being necessarily malicious.
\end{enumerate}

While the two seem similar, we cover them separately, because they are performed by different types of attackers
and the detection mechanisms are different.
We would like to investigate the different behaviors of peers and how to deal with them.
How severe should the punishment for each type of attack be?

\textbf{Data availability degradation} could be caused by the following peer behaviors:
\begin{enumerate}
    \item \textbf{Not storing the data} --- the peer does not store the data at all.
    \item \textbf{Peer goes offline permanently} --- the peer goes offline at some point in the future after accepting to store the data.
    \item \textbf{Peer goes offline temporarily} --- the peer goes offline for a short period of time.
        This could be a peer that is restarting or lost connection to the network.
        This is not necessarily a malicious peer, but it should be accounted for,
        because malicious peers could be performing an eclipse attack.
    \item \textbf{The peer is slow to respond} --- the peer is slow to respond to requests.
        This could happen when a peer is overloaded or has a slow connection.
        It could also be a malicious peer that is not storing the data and tries to request it
        from another peer in the network, before answering the request.
\end{enumerate}

\textbf{Data integrity degradation} could be caused by the following peer behaviors:
\begin{enumerate}
    \item \textbf{Storing corrupted data} --- the peer stores corrupted data.
        It could be that the peer is trying to save space by storing partial data
        or is trying to disrupt the network.
        Or perhaps the data has become corrupt over time without the peer being malicious.
    \item \textbf{Storing corrupted metadata} --- the peer stores the correct data, but the metadata is corrupted.
        This could mean storing the wrong expiry date or the wrong key.
        This kind of behavior either wouldn't affect the operation of the network or it would fall
        under the other types of behaviors.
        For example, if the peer is storing the wrong key, it would be the same as not storing the data,
        as querying for the original key would result in no response.
\end{enumerate}

The impact of all of these behaviors on the network is always dependent on the time it takes to detect them.
If the time to verify a piece of data is too long, a malicious peer could delete it and then
request it from another peer in the network just before the audit.
Thereby passing the audit, but not contributing to the network.
While this is a possible attack, it is an expensive one to execute,
because the attacker needs to perform a lot of work to pass the audit.
The attacker also needs to contribute considerable amount of their bandwidth to execute such an attack.
This is precisely the reason why we want lightweight audits for the auditor and heavier audits for the auditee.
For the above reasons, we do not consider this attack in our analysis.

\section{Solving the integrity problem}
\label{section:solving-the-integrity-problem}

To solve the integrity problem, we need to be able to check if the data is stored correctly.
To achieve this we need a verification/auditing mechanism, which is efficient and secure.
We will use a Proof of Retrievability (PoR) scheme, which will allow us to efficiently check the 
integrity of the data stored in the network.
PoR schemes are a type of cryptographic scheme that allows a verifier to check if a node
stores the data it claims to store.
The verifier does not need to download the whole file, but only exchange a few messages, which are much smaller.

Once we have a method to check the integrity of the data, we need someone to perform this check.
We will use the nodes in the network to perform the checks.
It makes sense for honest nodes, which are users of the network, to do audits,
as they have a vested interest in the integrity of the data.
However, we also need to make sure that malicious nodes are also doing audits.

We can achieve this by using rewards and penalties.
If a node is found to be storing data incorrectly (by failing an audit), we will penalize the node.
If a node is found to not be performing audits, we will penalize it.
On the other hand, if a node is storing data correctly (by passing an audit), we will reward the node.
And if a node is performing audits, we will reward it.

One way to penalize nodes is to remove them from the network.
However, it would be better if we could have lighter penalties.
Here we are proposing a reputation system based on a ledger store, e.g., a blockchain.
We will use a blockchain to store the reputation of the nodes.
This will allow us to have a light penalty system, as we can downgrade the reputation of the node
instead of removing it from the network.
Using a ledger store allows us to have a transparent and immutable record of the reputation of the nodes.

Using a reputation system will also allow us to reward nodes for performing audits.
Successfully storing a file and passing audits will increase the reputation of the node.
Successfully performing audits will also increase the reputation of the node.
Failing to store a file or failing to perform an audit will decrease the reputation of the node.
In the \ref{section:evaluation} we will discuss how very high or low reputation nodes will be treated.

We know how to perform audits, but we also need to know how to make sure none of the
audits are faked.
To solve this, we would require the nodes performing an audit to record the results of the audit and keep
it in a ledger store.
When a node is suspicious of the correctness of the audit, it can check the ledger store to see if the audit
was performed correctly.
This could also be used to check if the audits are being performed at all.

\section{Using Proof of Retrievability (PoR) to prevent attacks}

Preventing a Data availability attack can be done by querying the peer whether it has the data.
In this case we assume that the peer is honest and always returns a simple answer whether it has the data or not.
The assumption that a peer is honest is naive, but allows us to easily check if the peer is responsive.
One of the reasons we want to separate this attack is, so we can analyze peers going offline separately.
We will discuss how to handle temporary and permanent offline peers in the evaluation section.

Preventing a Data integrity attack can be done by requesting the peer to return the data and checking if
the data is correct.
This is a costly operation, because the data could be large, and the network could be slow.
This is a stronger requirement for the network, because peers performing this attack are a superset
of the peers performing the data availability attack.

In order to address data integrity attacks, we make use of Proof of Retrievability (PoR).
PoR is a cryptographic protocol that allows a peer to prove to a verifier that it is storing a file.
The verifier does not need to know the file, but only a small secret that is generated from the file.
The verifier can then challenge the peer to prove that it is storing the file by sending a challenge and
comparing the response to an expected response.
Both the challenge and the response are small.
The exact size depends on the PoR protocol used, and in our case is $O(\sqrt{N})$ \cite{poralgebra} for
both the challenge and the response, where $N$ is the size of the file in bytes.

Since the PoR protocol is cheap to execute both on the network and on the computation side of the verifier,
it can be used to address both types of attacks.

PoR has two versions --- default and public.
The default version is when the Verifier requires some form of secret to be able to run the audit.
The public version is when the Verifier does not require any secret to run the audit.
If the default version is used it is important to not reveal the secret to the Keeper,
because the Keeper could then use the secret to generate the response without storing the data.
Ideally we want to use the public version of the PoR protocol whenever possible.

In \autoref{section:reputation-system} we discuss how the PoR protocol can be used in conjunction with
a reputation system to keep track of the behavior of the peers in the network long-term.
However, there is another approach that Filecoin \cite{filecoin} uses, which is to use 
Proof of Spacetime.
Proof of Spacetime runs the verification protocol on regular occasions and uses chained hashes or signatures
to combine the old proofs into a single proof.

\section{Reputation system}
\label{section:reputation-system}

To keep track of the behavior of the peers in the network, we use a reputation system.
In short, we want well-behaved peers to have a high reputation and malicious peers to have a low reputation.
Also, well performing peers should have a higher reputation than poorly performing peers.
e.g., a peer that responds slowly or fails audits because their hard drive malfunctioned should
have a lower reputation than a peer that is always online and responds quickly to requests.
While this might seem punishing to the peer that has a malfunctioning hard drive,
it is important to keep the network healthy and to prevent the peer from causing more damage to the network.
Peers with higher reputation are prioritized by the network, while peers with lower reputation are
avoided and potentially removed from the network.
A key part of designing the reputation system is defining how much reputation is adjusted in each case.
First, let us look into what situations lead to reputation adjustment, and then we can
analyze how much reputation should be adjusted in each case.

Reputation is increased when a peer is behaving as expected,
i.e., storing data and providing data, as well as performing audits, all lead to an increase in reputation.

Reputation should be decreased in the case of failure to comply with the network rules.
A peer that is not storing data should have their reputation reduced on each audit,
until their reputation is so low that they are not trusted anymore and are removed from the network.
A peer that goes offline permanently would fall under the same category, as it would fail all its audits.
A peer that goes offline temporarily should be punished for the failed audits,
but the punishment should be such that for a short period of being offline the peer should not use 
too much of its reputation.
When a peer is slow to respond to requests, the audit is considered failed and the peer has
its reputation reduced as if it was not storing the data.

For the data integrity attacks, and in particular for storing corrupted data,
the punishment should be the same as for the data availability attacks.

The questions we need to answer are:
\begin{enumerate}
    \item How many audits can we perform in parallel?
    \item What is the performance impact of the audits on the network?
    \item How do we balance the punishment so that a peer that has been offline for a short period of time
        is not punished too much?
    \item What is a short period of time to be offline?
    \item Can we make the punishment scale with time? The longer the peer is offline, the more severe the punishment.
    \item What does it mean for a peer to be slow to respond?
\end{enumerate}

We have mostly answered the last question in our previous work, where we have evaluated the performance of
the PoR scheme we have implemented.
For small enough files, we cannot guarantee that the peer is storing the data
and not just fetching it from another peer in the network.
But to answer the question fully, we also need to account for the network latency.
If we look at the ping times between London, UK and Christchurch, New Zealand,
we can see that the average ping time is around 275ms \cite{pingtimes}.
These two cities give us a rough estimate of the maximum distance between two peers in the network,
as they are on the opposite sides of the globe.
These results are achieved under ideal conditions, and the actual ping time could be much higher.
We propose 500ms as the maximum time a peer can take to respond to a request.
This is almost 2 times the average ping between two very distant cities, so it should cover all cases.
If we set such reasonable timeouts, we can be sure that the peer is not
fetching the data from another storage, because cheap storage is excruciatingly slow.

This time is not accounting for the time it takes to process the request, but only the time it takes for the
request to reach the peer.
The time to process the request is the sum for reading the file from disk and executing the PoR protocol.
The time to execute the PoR protocol is around 40ms for a 100 MB file as our benchmarks show in \autoref{fig:por-keeper}.
The time to read a 100 MB file from disk is around 14ms on an SSD and 200ms on an HDD, if we
use the average read speed of 7000 MB/s for an SSD and 500 MB/s for an HDD.
Assuming that most peers use HDD for cheap storage, we shall use the 200ms time to read a 100MB file from disk.
Adding these up, we should expect a Keeper's response to take 500ms + 250ms for each 100 MB of data.

\begin{figure}
  \myfloatalign
  \begin{tikzpicture}
    \begin{axis}[
      xlabel={File size (MB)},
      ylabel={Time (ms)},
      grid=major,
      width=0.8\textwidth,
      height=6cm,
    ]
    \addplot[mark=., blue] coordinates {
      (1, 0.21614)
      (10, 3.002728)
      (100, 38.900041)
    };
    \end{axis}
  \end{tikzpicture}
  \caption[]{Time required to create the challenge response for PoR on the Keeper side, without reading the file from disk.}
  \label{fig:por-keeper}
\end{figure}

\subsection{Attacks on the reputation system}

One of the questions we want to answer in this thesis is if a reputation system based
on a ledger store is secure.
In other words, can we use the ledger to store the reputation of the peers in the network and be sure that
the reputation is not tampered with,
and does introducing a reputation system based on a ledger store
perhaps introduce new vulnerabilities to the system?
To answer this question we need to talk about possible attacks on the reputation system.

Ranking peers in the network based on their reputation means that 
having higher reputation gives merits to a peer.
Therefore, peers might try to game the system in order to increase their reputation.

We focus on the following attacks:
\begin{enumerate}
    \item \textbf{Increasing reputation attack} --- a peer tries to increase its reputation.
    \item \textbf{Decreasing reputation attack} --- a peer tries to decrease another peer's reputation.
\end{enumerate}

\textbf{Increasing reputation attack} is when a peer tries to increase its reputation by
cheating the system.
The reputation is stored in a ledger, which means that any changes to the data are recorded.
If a peer tries to increase its reputation, the other peers in the network can detect this by
checking the ledger's history.
This leaves the attacker one option - award itself reputation, i.e., a Sybil attack.
Since reputation is only awarded for storing data and performing audits, the attacker could join the network
with multiple identities and perform audits on itself.
This wouldn't work, because audits are performed on a rotating basis, i.e., each cycle a peer audits only a 
certain subset of the network.
If an attacker tries to audit itself not following the cycle, the other peers in the network can detect this,
because the audits are stored in the ledger as well and can be checked by other peers in the network.
This leads us to the situation where if a peer tries to perform a Sybil attack and cheats, it is detected by
the network.
If the peer tries to perform a Sybil attack and doesn't cheat,
it simply contributes to the network as any other peer.

\textbf{Decreasing reputation attack} is when a peer tries to decrease another peer's reputation.
We can apply the same reasoning as for the increasing reputation attack.
Directly increasing another peer's reputation is not possible, because the ledger is immutable and keeps track of
all the changes.
The second way to decrease reputation is to perform audits on the peer and fail them.
Since the audits' results are stored in the ledger, the other peers in the network can check the results,
and see if the peer is being honest.

As a conclusion - while attacks are possible, they are easily detected by the network
and do not pose a threat to the system as a whole.

\section{Audits}

Audits are a key part of the reputation system.
They check if the peers are storing the data they claim to store.
The audits are performed by the Verifier against the Keepers and the results are stored in the ledger.
The audit process follows the PoR protocol, where the Verifier sends a challenge to the Keeper,
and the Keeper responds with the proof, which the Verifier checks.

Verifiers perform audits on a rotating basis, i.e., each cycle a Verifier is responsible for auditing
a subset of the files in the network.
e.g., If we have two Verifiers, in cycle one Verifier A audits the first half of the key space,
and Verifier B audits the second half.
In cycle two, they switch --- Verifier A audits the second half and Verifier B audits the first half.
The length of the cycle depends on how much data is in the system --- this determines
how much time it takes to audit all the files in the network.

Upon completing the audit of all the files, the Verifier stores the results in the ledger.
The results are signed and timestamped by the Verifier to
ensure they can be checked by other peers in the network.
The result of an audit has a similar structure to the following JSON object:
\begin{verbatim}
  {
      "keeper": "A",
      "file": "1.txt",
      "result": "success",
      "cycle": 1,
      "time": "2020-01-01T00:00:00Z"
  }
\end{verbatim}

Upon a successful audit the Keeper gains reputation points.
Upon a failed audit the Keeper loses reputation points.
The Verifier also gains reputation points for performing the audit.

These 3 changes in reputation are stored in the ledger and can be checked by other peers in the network.
We discussed attacks where the Keeper aims to adjust its reputation in the previous section.
We will now look into how we can stop the Verifier from misbehaving.

The Verifier can misbehave in the following ways:
\begin{enumerate}
    \item \textbf{Not performing audits} --- the Verifier claims to perform audits, but does not.
    \item \textbf{Performing audits incorrectly} --- the Verifier performs audits, but does not check the results correctly.
    \item \textbf{Accusing the Keeper of not storing the data} --- the Verifier accuses the Keeper of not storing the data, but the Keeper is storing the data.
\end{enumerate}

\textbf{Not performing audits} can be caught by inspecting the result of the audits,
which is stored in the ledger.
In other words, in a given cycle, when a Verifier is auditing a set of Keepers,
it also audits the Verifiers that were previously responsible for auditing the Keepers.
These audits are used to compare the results of the current Verifier
with the results of the previous Verifiers, which can be found in the ledger.
There are two ways to audit the previous Verifiers.
One is to audit the Verifiers from the previous few cycles.
And the second is to randomly sample the Verifiers to audit,
i.e., randomly sample previous cycles to audit.
Auditing the Verifiers from the previous few cycles is more secure short term
as it ensures that the most recent audits were performed correctly.
However, if we want to check old cycles as well, it becomes more expensive ---
for each previous cycle, we have to fetch the results from the ledger, compare them,
and write the comparison results back to the ledger.
Random sampling is cheaper to execute, but does not provide strict guarantees
that the audits were performed correctly in the most recent cycles.
If we have very few Verifiers in the network, we can use the first method,
as the cycle length will be short and the cost of auditing the previous Verifiers would be low.
If we have many Verifiers in the network, we can use the second method,
which will not provide strict guarantees, but will be cheaper to execute.
Finally, we can combine the two methods to get the best of both worlds.

\textbf{Performing audits incorrectly} can be caught in the same way as not performing audits.

\textbf{Accusing the Keeper of not storing the data} can be caught in the next cycle
when the next Verifier audits the same Keeper.
If the Keeper is not storing the data, the audit would fail, and the Keeper would have its reputation reduced.
If the Keeper is storing the data, the audit would pass, and the Verifier that accused the Keeper
would have its reputation reduced.
In theory this can put the Keeper in a position of power, where it can fail the audit of the Verifier
and in the following cycle pass the audit, falsely accusing the first Verifier.
This is not a viable attack since the Keeper would need to sacrifice its reputation to perform the attack
(on a failed audit the Keeper loses reputation).
It becomes a game of who has more reputation to lose.
At this point, if the malicious Keeper(s) have more reputation to lose than the Verifier(s),
they would also have full control of the network,
since with more reputation they would have the majority of the network's trust.
While this is a possible attack, we will not consider it in our analysis,
because it is a very expensive attack to perform, and we do not consider
attacks where the malicious party has more than 50\% of the reputation in the network
or more than 50\% of the nodes in the network.

\subsection{Conclusion}

In conclusion, if we want to introduce Verifiers and allow each peer in the system to be a Verifier,
we need to ensure that the Verifiers are performing their duties correctly,
similarly to how we ensure that the Keepers are storing the data correctly.
In this case we cannot use PoR, but instead we can perform simple audits on the Verifiers,
by comparing the other Verifiers' results with the current Verifier's results.
We are assuming that the current Verifier is one that is honest and is performing the audits correctly.

The downside of this approach is that if there are more than 50\% malicious Verifiers in the network,
they can collude and cheat the system,
so we require the majority of the Verifiers to be honest.
In particular, it is possible for Keepers to attack the Verifiers by
selecting a Verifier and failing its audits.
On the following cycle, the Keeper would be audited by another Verifier,
and it would pass the audit, resulting in the first Verifier losing reputation.
While this is a possible attack, it can be prevented by introducing some form of
onion routing.
This would prevent the Keeper from knowing which Verifier is auditing it,
which would make the attack meaningless.

\section{Ledger}

The ledger is used as a catalog for the files stored in the network ---
it stores contracts, which state the key of the file, what peer is storing the file,
and the expiry date of the file.
The ledger also stores the reputation of the peers in the network.

In the previous section we discussed how the ledger is used to store the results of the audits.
These results could be very large, if we have many files in the network.
The most basic way to store the results is to store them as a list of JSON objects:

\begin{verbatim}
  {
      "keeper": "A",
      "file": "1.txt",
      "result": "success",
      "cycle": 1,
  }
\end{verbatim}

To optimize the storage of the results, we can store only the failed audits.
We can also only store the contract ID, which is a reference to the file being stored, and the peer ID
where it is stored.
Finally, we could store the results in the System itself and only store the signed hash of the results in the ledger.
This is particularly useful if we want to use a blockchain as the ledger, because then we want to
store as little data as possible in the blockchain, so it does not grow too large.

Ideally the ledger should be a blockchain as it is a form of distributed ledger,
that all peers in the network can access and check.

\subsection{Blockchain as a ledger}

Typical blockchains store transactions of tokens, where an amount of the coin or token
is transferred from one account to another.
In our case we want to store the results of the audits,
as well as the reputation of the peers in the network.

Let us discuss storing the reputation first.
We want to treat the token of the blockchain as reputation points.
When a peer gains reputation, it gains tokens, and when it loses reputation, it loses tokens.
The wealth of a peer is the same as its reputation.
We do not want to use the coin as a payment method, but as a form of trust.
Blockchains such as Filecoin \cite{filecoin} use the coin as payment for storing data.
We want to trade "trust".
In other words, when we want to store a file in the network, we pay trust to the Keeper,
increasing the trust of that Keeper if they store the file.
Unlike typical blockchains where the coin is traded between users,
we want the coin to be traded between peers.
In essence, making the peers the real users of the blockchain.

\section{Augmenting the PoR protocol with receipts (rejected idea)}

One alternative idea to improve the audit process is to augment the PoR protocol with receipts.
After an audit the Keeper would receive a receipt from the Verifier and would store this
receipt locally.
The receipt indicates that the audit was performed and the result of it.
The receipt would also be signed by the Verifier.
This way the Keeper can prove to other peers that the audit was performed
in case the Verifier tries to accuse it of not storing the data.

The downside of this approach is that it increases the complexity of the system,
but does not provide much benefit.

Let us look at the four possible scenarios.
For each of them we shall assume the Verifier in the following cycle is honest
and performs the audit correctly.
If it is not honest, and lies about the result of the audit,
then the next honest Verifier will catch it and the malicious Verifier will lose reputation.
In other words it is okay to have a chain of malicious Verifiers,
as long as it ends with an honest Verifier that will catch the malicious Verifiers
(assuming the system is not fully consisting of malicious Verifiers).

\textbf{Keeper has the data \& audit succeeds} ---
This is the happy path where the Keeper has the data and the Verifier
writes in the ledger that the audit was successful.
This is the expected good behavior of nodes in the system.
The next Verifier in the next cycle (assuming honest) will also perform a successful audit.

\textbf{Keeper does not have the data \& audit fails} ---
The other almost happy path.
Both the Keeper and the Verifier are honest, but the Keeper has lost the data.
The next Verifier in the next cycle will also perform an audit and see that the Keeper does not have the data.
This is the expected behavior of the Verifiers in the system.

\textbf{Keeper does not have the data \& audit succeeds} ---
This is the case where the Keeper does not have the data, but the Verifier writes in the ledger
that the audit was successful.
This is a case where the Verifier is malicious and tries to increase the Keeper's reputation.
The next Verifier in the next cycle will also perform an audit and see that the Keeper does not have the data.
Hence, it will reduce the first Verifier's reputation.
This is the case where the malicious peers try to increase each other's reputation,
however, if there are more honest peers in the network, they will lose more reputation than they give each other.

\textbf{Keeper has the data \& audit fails} ---
This is the case where the Keeper has the data, but the Verifier writes in the ledger
that the audit failed.
This is the interesting case, that the receipt would aim to solve.
The Verifier is malicious and tries to decrease the Keeper's reputation.
In the next cycle, the next Verifier will also perform an audit and see that the Keeper has the data.
Hence, it will reduce the first Verifier's reputation.
In essence, the first Verifier will trade its reputation for the Keeper's reputation.
This falls under the same category as the previous case, where the malicious peers try to increase each other's reputation,
