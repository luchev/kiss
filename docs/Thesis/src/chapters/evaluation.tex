\chapter{Evaluation}
\label{chapter:evaluation}

Before we evaluate the system, we will make a few notes that apply to the evaluation of the system as a whole.

\section{Notes}

All the tests and benchmarks were performed on a single machine.
The machine has 16GB of RAM, an M1 Apple Silicon processor 3200 MHz, and a 512GB SSD.
Due to these specs, the benchmarks are run with files between 1MB and 100MB.
We ran benchmarks with files up to 1GB and the results were consistent with the results from the smaller files,
i.e., the algorithms exhibit the same behavior, but rerunning the benchmarks takes a lot of time.

The system is written in Rust, which means no garbage collection or runtime overhead,
which we can see from running 1000 instances in \autoref{fig:1000-instances}.
Because the system consumes almost no resources when idle,
we can run many instances on a single machine.
We will not account for the overhead of the system being idle in the benchmarks.

\begin{figure}
    \centering
    \includegraphics[width=350pt]{gfx/1000-instances.png}
    \captionof{figure}{Starting 1000 instances}
    \label{fig:1000-instances}
\end{figure}

\section{Proof of Retrievability (PoR)}
\label{section:por-evaluation}

In this section, we will evaluate how the PoR system performs in a real-world scenario.
We have discussed how PoR is used to ensure the integrity of the data stored on the nodes.
Now, we will evaluate how viable is it to use PoR in the system.
Does it adhere to the \autoref{section:requirements}?
And is it fast enough to be used in a real-world scenario?

\subsection{How many audits can we perform in parallel?}

The audit is a two-step process.
First, the Verifier sends the challenge to the Keeper and then the Keeper responds with the proof.

We expect the load on the Verifier to be negligible, while the load on the Keeper to be significant.
The Verifier should do as little work as possible,
because we want to perform as many audits as possible in parallel.
This way we can ensure that audits are performed often enough to ensure the integrity of the data.
The load on the Keeper is expected to be high,
because the Keeper needs to read the file from disk and execute the PoR algorithm.
Ideally that algorithm will be linear in time complexity in the size of the file.

The Verifier performs 2 operations --- sending the challenge and verifying the proof.
We benchmarked these operations and the results are shown in \autoref{fig:por-verifier-create-challenge}
and \autoref{fig:por-verifier-audit-challenge}.

The results show that generating the challenge and auditing are both very fast operations ---
for a 100MB file, the Verifier can generate the challenge in 0.027ms and
audit the proof in 0.137ms.

We can observe something interesting the in the results ---
the graph looks almost logarithmic, instead of linear.
This is due to cache locality since both the operations are executing code on sequential memory addresses.
Both the challenge and the response are essentially a vector of numbers,
which is a very cache-friendly data structure.

\begin{figure}
  \myfloatalign
  \begin{tikzpicture}
    \begin{axis}[
      xlabel={File size (MB)},
      ylabel={Time (ms)},
      grid=major,
      width=0.8\textwidth,
      height=6cm,
    ]
    \addplot[mark=., blue] coordinates {
      (1, 0.002736)
      (10, 0.008674)
      (100, 0.027069)
    };
    \end{axis}
  \end{tikzpicture}
  \caption{Time required for the Verifier to create the challenge for PoR.}
  \label{fig:por-verifier-create-challenge}
\end{figure}

\begin{figure}
  \myfloatalign
  \begin{tikzpicture}
    \begin{axis}[
      xlabel={File size (MB)},
      ylabel={Time (ms)},
      grid=major,
      width=0.8\textwidth,
      height=6cm,
    ]
    \addplot[mark=., blue] coordinates {
      (1, 0.013733)
      (10, 0.043281)
      (100, 0.137354)
    };
    \end{axis}
  \end{tikzpicture}
  \caption{Time required for the Verifier to audit the challenge response PoR.}
  \label{fig:por-verifier-audit-challenge}
\end{figure}

The Keeper performs 1 operation --- generating the proof.
This is done by reading the file from disk and running the PoR algorithm on it.
The PoR algorithm is expected to be linear in time complexity in the size of the file.
We benchmarked this operation and the results are shown in \autoref{fig:por-keeper-challenge-generate}.

The operation gets slower as the file size increases.
In theory the time complexity should be linear, but it is superlinear in practice.
This is most likely caused by the unoptimized matrix multiplication algorithm that we are using to
implement PoR.

\begin{figure}
  \myfloatalign
  \begin{tikzpicture}
    \begin{axis}[
      xlabel={File size (MB)},
      ylabel={Time (ms)},
      grid=major,
      width=0.8\textwidth,
      height=6cm,
    ]
    \addplot[mark=., blue] coordinates {
      (1, 0.21614)
      (10, 3.002728)
      (100, 38.900041)
    };
    \end{axis}
  \end{tikzpicture}
  \caption{Time required to create the challenge response (proof) for PoR on the Keeper side, without reading the file from disk.}
  \label{fig:por-keeper-challenge-generate}
\end{figure}

\subsection{Is PoR the limiting factor in the system?}

One of the main questions we want to answer in this thesis is whether the validation component
affects the system's performance.
From the results we can see that while the evaluation aspect introduces more workload,
however, it is within reasonable levels.

In the previous section we saw that the Verifier can generate the challenge and audit the proof very quickly.
Since the speed of reading from an SSD is on average 7000 MB/s, the PoR protocol in the Verifier
will not affect the system's performance.
It is more likely, that the network connection between the Verifier and the Ledger will be the limiting factor,
but we will not evaluate network speeds as they are out of the scope of this thesis.

The Keeper can generate the proof at a rate of 38ms for a 100 MB.
This is faster than the access speed of an HDD, which is around 500 MB/s or 200ms for a 100 MB.
However, it is slower than the access speed of an SSD, which is around 7000 MB/s or 14ms for a 100 MB.
Whether the PoR protocol is the limiting factor or not depends on what kind of storage the Keeper is using.

In conclusion, the Verifier can send challenges in parallel, but the Keeper nodes will still be
the limiting factor in the system.
% 10 1kb
% real    0m0.804s
% user    0m0.196s
% sys     0m0.167s

% 100 1kb
% real    0m5.146s
% user    0m1.779s
% sys     0m1.264s

\section{How fast are malicious behaviors detected?}

All files are verified each cycle.
The Verifiers split the work of verifying the files based on the file's key.
As an example, if we have two peers (2 Keepers and 2 Verifiers),
the Verifiers would split the files of each Keeper and take turns verifying them
during alternating cycles.
This is illustrated in \autoref{fig:2-verifiers}.

\begin{figure}
    \centering
    \includegraphics[width=350pt]{gfx/2-verifiers.png}
    \captionof{figure}{Splitting the ID space between two Verifiers}
    \label{fig:2-verifiers}
\end{figure}

This means that when a peer becomes malicious they have at most one cycle of time,
before they are detected.
We are running the benchmarks in \autoref{table:storage_detection_times} with a 5s cycle time
and various file sizes.
The 5 seconds period is arbitrarily chosen to give us initial results.
The cycle time when running the system in production should be set to a value proportional to
the amount of data stored in the network.
Based on the results from the benchmarks in \autoref{section:por-evaluation},
we can conclude that ...
\wtf{for each 100k files added to the system, we need to increase the cycle time by...,
for each 1GB of data stored in the system, we need to increase the cycle time by...,
for each X number of peers in the network, we need to increase the cycle time by...}

\wtf{todo: run benchmarks and update times}

\begin{table}
  \myfloatalign
  \pgfplotstabletypeset[
  every head row/.style={ before row=\toprule, after row=\midrule },
  every last row/.style={ after row=\bottomrule },
%  every even row/.style={ %Define alternating row colouring
%    before row={\rowcolor[gray]{0.95}}
%  },
  columns={x, avgy, stddevpp },
  columns/npeers/.style={ column name=\textsc{Number of peers}, dec sep align, fixed, fixed zerofill, precision=2 },
  columns/data/.style={ column name=\textsc{Data stored}, dec sep align, fixed, fixed zerofill, precision=2 },
  columns/time/.style={ column name=\textsc{Time without Verifier running}, dec sep align, fixed, fixed zerofill, precision=1 },
  columns/timever/.style={ column name=\textsc{Time with Verifier running}, dec sep align, fixed, fixed zerofill, precision=1 },
  ]{./data/store.csv}
  \caption{TODO caption}
  \label{tab:pretty-table}
\end{table}


\begin{table}[h!]
\centering
\begin{tabular}{|>{\centering\arraybackslash}m{4cm}|>{\centering\arraybackslash}m{4cm}|>{\centering\arraybackslash}m{4cm}|}
\hline
\textbf{Files stored} & \textbf{Time to store all files} & \textbf{Time to detect malicious behavior} \\ \hline
100x1kb & 0.0s & 0.0s \\ \hline
1000x1kb & 0.0s & 0.0s \\ \hline
10000x1kb & 0.0s & 0.0s \\ \hline
100000x1kb & 0.0s & 0.0s \\ \hline
100x1mb & 0.0s & 0.0s \\ \hline
1000x1mb & 0.0s & 0.0s \\ \hline
10000x1mb & 0.0s & 0.0s \\ \hline
100x100mb & 0.0s & 0.0s \\ \hline
\end{tabular}
\caption{Storage and Detection Times for Various File Sizes with 5s cycle time.}
\label{table:storage_detection_times}
\end{table}

\section{Reputation system based on a ledger}

The results of the PoR are stored in a ledger, which acts as the source of truth for
the performance of the peers in the network.
While ideally we would like to have a decentralized ledger, for the sake of simplicity
we will use a centralized ledger in our evaluation.
For the purposes of evaluation we could have used a simple SQL database or a local file,
however, we used Immudb because it is what we used in the previous iterations of the system
and at the time we were exploring it as a potential solution for the ledger.

\subsection{Immudb performance}

Having a centralized ledger means a bottleneck in the system.
\wtf{Cover how many requests per second Immudb can handle.
Centralized is of course bad, but what is exactly limiting us when using a centralized ledger?}


\subsection{Reputation system based on a ledger}
\wtf{Answer the question from the hypothesis: 
    Is a reputation system based on a ledger a viable measure against malicious nodes?
}
\wtf{
    I think in this section I'll cover what are a ledger's strong points and guarantees that
    would be beneficial for the reputation system.
}

\section{Using Rust}

\wtf{TODO: MOVE THIS TO THE END}

\wtf{Cover what are the strong/weak points of using Rust for such a system.}

\section{Balancing the reputation system}

\subsection{Gaining and losing reputation as a Keeper}
\wtf{Cover how the reputation awards should be balanced against the punishments to ensure
that the system can maintain its integrity. We will assume that a node dropping to negative reputation
results in it being kicked out of the network.}

\begin{table}[h!]
\centering
\begin{tabular}{|>{\centering\arraybackslash}m{2cm}|>{\centering\arraybackslash}m{2cm}|>{\centering\arraybackslash}m{2cm}|>{\centering\arraybackslash}m{2cm}|>{\centering\arraybackslash}m{3cm}|}
\hline
\textbf{Malicious nodes} & \textbf{Initial reputation} & \textbf{Rewards} & \textbf{Punishments} & \textbf{Time for a peer to lose its reputation} \\ \hline
1\% & 0 & 0 & 0 & 0 \\ \hline
10\% & 0 & 0 & 0 & 0 \\ \hline
20\% & 0 & 0 & 0 & 0 \\ \hline
\end{tabular}
\caption{Reputation balancing and time required for a peer to lose all its reputation after becoming malicious.}
\label{table:storage_detection_times}
\end{table}

\subsection{Gaining and losing reputation as a Verifier}

\wtf{Do the same for checking the previous Verifier's audit as we do for the Keeper}


\section{TODO}
\wtf{Nothing has changed from here onwards}

\wtf{this section needs to be rewritten}

We need to evaluate the proper penalties and rewards for the nodes.
A good balance is needed between rewards and penalties to ensure that a node is
going to perform enough work for the system before being able to harm it.
Part of this evaluation will be:
\begin{enumerate}
    \item How much reputation points a node stakes when it stores a file?
    \item How much reputation points a node loses when it fails an audit?
    \item How much reputation points a node gains when it performs an audit?
    \item How much reputation points a node loses when it's discovered to not be performing audits?
    \item What is the starting reputation for new nodes?
    \item How do nodes increase their reputation at the very beginning?
    \item Does having high reputation give the node any benefits?
\end{enumerate}

Nodes with very high reputation could easily become malicious as they can absorb the penalties.
We need to solve this problem by either having a maximum reputation or by having the penalties be
percentage-based.

We need to answer the question of how often audits should be performed and how often nodes should
check the audit results of others.
The overhead of performing audits should be as low as possible, but the audits should be performed often enough
to ensure the integrity of the data.

Finding the balance between audits and the overhead of the audits, and the penalties and rewards for the nodes
is crucial for the success of the system.
We will evaluate the system by running it with different parameters and observing the behavior of the nodes.
We will discuss the details and the results of the evaluation in \ref{chapter:evaluation}.

\section{TODO reputation}
\wtf{TODO: I'm still playing around with this idea, I'm not sure if this is the best way to do it.
Perhaps the staked reputation will be returned gradually with each successful audit.
The whole idea of adding "staking" is to ensure the peer has a big hit in reputation from the beginning.
This way a new peer in the network cannot just accept a lot of files as soon as it joins and then go offline.}
Every time an audit is performed the reputation of the peer on which the audit was performed is adjusted.
If the audit is successful, the reputation of the peer is increased,
and if the audit fails, the reputation of the peer is decreased.
Finally, a peer is awarded reputation points for performing audits.
The increase and decrease numbers are configurable and can be adjusted to fit the network requirements.
We will discuss the different possibilities for adjusting the reputation in the \autoref{chapter:evaluation}

\wtf{TODO: notes from meeting:
give peers some leeway from being offline - give it some time to be offline before punishing.
take a look at the classic report for TABLES
at the end of the evaluation add a sumamry - is the system viable or should you use amazon,
is the system recommened for production for companies - for what use cases is it recommened,
basically add stuff something akin to a conclusion that then can be used in the conclusion chapter,
for tables - how many times were tests ran, what is the standard deviation, averages
in the appendix - how did I run the tests, how to rerun them,
in the appendix - organization of the repository,
(table example from the classic report - related work section with short and long text)
eval - if someone sees this PoR for the first time - how expensive is it, what is the overhead, why not use
amazon/other s3
por could be good for when we store files in s3 and we use por to check the integrity of the files
}
